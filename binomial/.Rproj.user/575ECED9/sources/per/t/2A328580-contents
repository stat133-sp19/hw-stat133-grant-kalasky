---
title: "Stats133-Lab12"
author: "Grant Kalasky"
date: "4/24/2019"
output: html_document
---

### Motivation

```{r}
# importing packages
library(XML)
library(xml2)
library(rvest)
library(magrittr)
```

```{r}
# Assemble url (so it fits on screen)
basket <- "https://www.basketball-reference.com"
gsw <- "/teams/GSW/2017.html"
gsw_url <- paste0(basket, gsw)

# download HTML file to your working directory
download.file(gsw_url, 'gsw-roster-2017.html')

# Read GSW Roster html table
gsw_roster <- readHTMLTable('gsw-roster-2017.html')
```

```{r}
# Assemble url (so it fits on screen)
basket <- "https://www.basketball-reference.com"
bos <- "/teams/BOS/2017.html"
bos_url <- paste0(basket, bos)

# download HTML file to your working directory
download.file(bos_url, 'bos-roster-2017.html')

# Read BOS Roster html table
bos_roster <- readHTMLTable('bos-roster-2017.html')
```

### Your Turn: Extracting HTML elements

```{r}
nba_html <- paste0(basket, "/leagues/NBA_2017.html")
xml_doc <- read_html(nba_html)

# "h1"
xml_doc %>%
  html_nodes("h1") %>%
  html_text() 

# "strong"
xml_doc %>%
  html_nodes("strong") %>%
  html_text()

# "button"
xml_doc %>%
  html_nodes("button") %>%
  html_text() 
```

### Your turn*

- Store the href attributes in a character vector `hrefs`.
```{r}
xml_tables <- xml_doc %>%
  html_nodes("table") %>%
  extract(1:2)

href <- xml_tables %>% 
  html_nodes("a") %>%
  html_attr("href")
href
```

- Use string manipulation functions to create a character vector `teams` that contains just the team abbreviations: e.g. `"BOS", "CLE", "TOR", ...`
```{r}
library(stringr)

teams <- str_extract(href, "[A-Z]+")
teams
```

- Create a character vector `files` with elements: `"BOS-roster-2017.csv", "CLE-roster-2017.csv", "TOR-roster-2017.csv", ...`
```{r}
files <- paste0(teams, "-roster-2017.csv")
```

- Use the object `basket` and the first element of `hrefs` (i.e. `hrefs[1]`) to assemble a `team_url` like the one used for gsw_url
```{r}
basket <- "https://www.basketball-reference.com"
team1 <- "/teams/GSW/2017.html"
team_url <- paste0(basket, href[1])
```


- Read the html document of `team_url`.
```{r}
# download HTML file to working directory
download.file(team_url, html_files[16])
```

- Use `html_table()` to extract the content of the html table as a data frame called `roster`
```{r}
roster <- readHTMLTable(team_url)
```

- Store the data frame in a csv file: `"BOS-roster-2017.csv"`
```{r}
write.csv(roster, file = "GSW-roster-2017.csv")
```

Having making sure that your code above works, now generalize it to more teams. In theory, your code should be able to collect all 30 roster tables. However, since everyone will be making constant requests to the basketball-reference website at the same time, write code that scrapes a couple of roster tables (e.g. 5 or 7 teams).

```{r}
html_files <- paste0(teams, "-roster-2017.html")
```

```{r}
samples <- sample(30, 5, replace = FALSE)

for (i in samples) {
  team_url <- paste0(basket, href[i])
  file_path <- files[i]
  download.file(team_url, html_files[i])
  roster <- readHTMLTable(html_files[i])
  write.csv(roster, files[i])
}
```

### Challenge*

Using all the saved csv files, how would you build a global table containing the extracted rosters, in a way that this table would also have a column for the team?

```{r}
data1 <- read.csv("BRK-roster-2017.csv", header = TRUE)
data2 <- read.csv("IND-roster-2017.csv", header = TRUE)
data3 <- read.csv("LAL-roster-2017.csv", header = TRUE)
data4 <- read.csv("MIN-roster-2017.csv", header = TRUE)
data5 <- read.csv("WAS-roster-2017.csv", header = TRUE)

data1 <- as.data.frame(data1)
data2 <- as.data.frame(data2)
data3 <- as.data.frame(data3)
data4 <- as.data.frame(data4)
data5 <- as.data.frame(data5)

data1$team <- "BRK"
data2$team <- "IND"
data3$team <- "LAL"
data4$team <- "MIN"
data5$team <- "WAS"

temp_df <- merge(data1, data2, all = TRUE)
temp_df <- merge(temp_df, data3, all = TRUE)
temp_df <- merge(temp_df, data4, all = TRUE)
nba_df <- merge(temp_df, data5, all = TRUE)
nba_df
```
